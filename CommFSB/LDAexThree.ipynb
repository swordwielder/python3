{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the sample data set\n",
    "\n",
    "## About the Iris dataset\n",
    "For the following tutorial, we will be working with the famous “Iris” dataset that has been deposited on the UCI machine learning repository\n",
    "(https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "**Reference:** Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "The iris dataset contains measurements for 150 iris flowers from three different species.\n",
    "\n",
    "The three classes in the Iris dataset:\n",
    "\n",
    "Iris-setosa (n=50)\n",
    "Iris-versicolor (n=50)\n",
    "Iris-virginica (n=50)\n",
    "The four features of the Iris dataset:\n",
    "\n",
    "sepal length in cm\n",
    "sepal width in cm\n",
    "petal length in cm\n",
    "petal width in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(4),\n",
    "                  ('sepal length in cm',\n",
    "                  'sepal width in cm',\n",
    "                  'petal length in cm',\n",
    "                  'petal width in cm', ))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "145                 6.7                3.0                 5.2   \n",
       "146                 6.3                2.5                 5.0   \n",
       "147                 6.5                3.0                 5.2   \n",
       "148                 6.2                3.4                 5.4   \n",
       "149                 5.9                3.0                 5.1   \n",
       "\n",
       "     petal width in cm     class label  \n",
       "145                2.3  Iris-virginica  \n",
       "146                1.9  Iris-virginica  \n",
       "147                2.0  Iris-virginica  \n",
       "148                2.3  Iris-virginica  \n",
       "149                1.8  Iris-virginica  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "    header=None,\n",
    "    sep=',',\n",
    "    )\n",
    "df.columns = [l for i,l in sorted(feature_dict.items())] + ['class label']\n",
    "df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since it is more convenient to work with numerical values, we will use the LabelEncode from the scikit-learn library to convert the class labels into numbers: 1, 2, and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c503960e9a36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df[[0,1,2,3]].values\n",
    "y = df['class label'].values\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y) + 1\n",
    "\n",
    "label_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms and feature selection\n",
    "### Just to get a rough idea how the samples of our three classes ω1, ω2 and ω3 are distributed, let us visualize the distributions of the four different features in 1-dimensional histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bc7ddda10b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# set bin sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmin_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmax_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n",
    "\n",
    "for ax,cnt in zip(axes.ravel(), range(4)):  \n",
    "\n",
    "    # set bin sizes\n",
    "    min_b = math.floor(np.min(X[:,cnt]))\n",
    "    max_b = math.ceil(np.max(X[:,cnt]))\n",
    "    bins = np.linspace(min_b, max_b, 25)\n",
    "\n",
    "    # plottling the histograms\n",
    "    for lab,col in zip(range(1,4), ('blue', 'red', 'green')):\n",
    "        ax.hist(X[y==lab, cnt],\n",
    "                   color=col,\n",
    "                   label='class %s' %label_dict[lab],\n",
    "                   bins=bins,\n",
    "                   alpha=0.5,)\n",
    "    ylims = ax.get_ylim()\n",
    "\n",
    "    # plot annotation\n",
    "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    ax.set_ylim([0, max(ylims)+2])\n",
    "    ax.set_xlabel(feature_dict[cnt])\n",
    "    ax.set_title('Iris histogram #%s' %str(cnt+1))\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "axes[0][0].set_ylabel('count')\n",
    "axes[1][0].set_ylabel('count')\n",
    "\n",
    "fig.tight_layout()       \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA in 5 steps\n",
    "## After we went through several preparation steps, our data is finally ready for the actual LDA. In practice, LDA for dimensionality reduction would be just another preprocessing step for a typical machine learning or pattern classification task.\n",
    "\n",
    "\n",
    "Step 1: Computing the d-dimensional mean vectors\n",
    "In this first step, we will start off with a simple computation of the mean vectors mmi, (i=1,2,3) of the 3 different flower classes:\n",
    "\n",
    "mmi=⎡⎣⎢⎢⎢⎢⎢μωi(sepal length)μωi(sepal width)μωi(petal length)μωi(petal width)⎤⎦⎥⎥⎥⎥⎥,withi=1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ba3cb7a275b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmean_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmean_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Vector class %s: %s\\n'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "\n",
    "mean_vectors = []\n",
    "for cl in range(1,4):\n",
    "    mean_vectors.append(np.mean(X[y==cl], axis=0))\n",
    "    print('Mean Vector class %s: %s\\n' %(cl, mean_vectors[cl-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Vector class 1: [ 5.006  3.418  1.464  0.244]\n",
    "\n",
    "Mean Vector class 2: [ 5.936  2.77   4.26   1.326]\n",
    "\n",
    "Mean Vector class 3: [ 6.588  2.974  5.552  2.026]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Computing the Scatter Matrices\n",
    "Now, we will compute the two 4x4-dimensional matrices: The within-class and the between-class scatter matrix.\n",
    "\n",
    "2.1 Within-class scatter matrix SW\n",
    "The within-class scatter matrix SW is computed by the following equation:\n",
    "\n",
    "SW=∑i=1cSi\n",
    "where\n",
    "Si=∑xx∈Din(xx−mmi)(xx−mmi)T\n",
    "(scatter matrix for every class)\n",
    "\n",
    "and mmi is the mean vector\n",
    "mmi=1ni∑xx∈Dinxxk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W = np.zeros((4,4))\n",
    "for cl,mv in zip(range(1,4), mean_vectors):\n",
    "    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every class\n",
    "    for row in X[y == cl]:\n",
    "        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors\n",
    "        class_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "print('within-class Scatter Matrix:\\n', S_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 b\n",
    "Alternatively, we could also compute the class-covariance matrices by adding the scaling factor 1N−1 to the within-class scatter matrix, so that our equation becomes\n",
    "\n",
    "Σi=1Ni−1∑xx∈Din(xx−mmi)(xx−mmi)T.\n",
    "\n",
    "and SW=∑i=1c(Ni−1)Σi\n",
    "where Ni is the sample size of the respective class (here: 50), and in this particular case, we can drop the term (Ni−1) since all classes have the same sample size.\n",
    "\n",
    "However, the resulting eigenspaces will be identical (identical eigenvectors, only the eigenvalues are scaled differently by a constant factor).\n",
    "\n",
    "2.2 Between-class scatter matrix SB\n",
    "The between-class scatter matrix SB is computed by the following equation:\n",
    "\n",
    "SB=∑i=1cNi(mmi−mm)(mmi−mm)T\n",
    "where\n",
    "mm is the overall mean, and mmi and Ni are the sample mean and sizes of the respective classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eb3886bdf0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moverall_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mS_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_vec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "S_B = np.zeros((4,4))\n",
    "for i,mean_vec in enumerate(mean_vectors):  \n",
    "    n = X[y==i+1,:].shape[0]\n",
    "    mean_vec = mean_vec.reshape(4,1) # make column vector\n",
    "    overall_mean = overall_mean.reshape(4,1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "print('between-class Scatter Matrix:\\n', S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Solving the generalized eigenvalue problem for the matrix S−1WSB\n",
    "### Next, we will solve the generalized eigenvalue problem for the matrix S−1WSB to obtain the linear discriminants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(4,1)   \n",
    "    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
    "    print('Eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "Depending on which version of NumPy and LAPACK we are using, we may obtain the matrix W with its signs flipped. Please note that this is not an issue; if v is an eigenvector of a matrix Σ, we have\n",
    "\n",
    "Σv=λv.\n",
    "\n",
    "Here, λ is the eigenvalue, and v is also an eigenvector that thas the same eigenvalue, since\n",
    "\n",
    "Sigma(−v)=−−vΣ=−λv=λ(−v).\n",
    "\n",
    "After this decomposition of our square matrix into eigenvectors and eigenvalues, let us briefly recapitulate how we can interpret those results. As we remember from our first linear algebra class in high school or college, both eigenvectors and eigenvalues are providing us with information about the distortion of a linear transformation: The eigenvectors are basically the direction of this distortion, and the eigenvalues are the scaling factor for the eigenvectors that describing the magnitude of the distortion.\n",
    "\n",
    "If we are performing the LDA for dimensionality reduction, the eigenvectors are important since they will form the new axes of our new feature subspace; the associated eigenvalues are of particular interest since they will tell us how “informative” the new “axes” are.\n",
    "\n",
    "Let us briefly double-check our calculation and talk more about the eigenvalues in the next section.\n",
    "\n",
    "Checking the eigenvector-eigenvalue calculation\n",
    "A quick check that the eigenvector-eigenvalue calculation is correct and satisfy the equation:\n",
    "\n",
    "AAvv=λvv\n",
    "where\n",
    "AA=S^(−1)WSB\n",
    "vv=Eigenvector\n",
    "λ=Eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eig_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3baee73abaf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0meigv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meig_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     np.testing.assert_array_almost_equal(np.linalg.inv(S_W).dot(S_B).dot(eigv),\n\u001b[1;32m      4\u001b[0m                                          \u001b[0meig_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meigv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          decimal=6, err_msg='', verbose=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eig_vals' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(4,1)\n",
    "    np.testing.assert_array_almost_equal(np.linalg.inv(S_W).dot(S_B).dot(eigv),\n",
    "                                         eig_vals[i] * eigv,\n",
    "                                         decimal=6, err_msg='', verbose=True)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Selecting linear discriminants for the new feature subspace\n",
    "### 4.1. Sorting the eigenvectors by decreasing eigenvalues\n",
    "Remember from the introduction that we are not only interested in merely projecting the data into a subspace that improves the class separability, but also reduces the dimensionality of our feature space, (where the eigenvectors will form the axes of this new feature subspace).\n",
    "\n",
    "However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1.\n",
    "\n",
    "So, in order to decide which eigenvector(s) we want to drop for our lower-dimensional subspace, we have to take a look at the corresponding eigenvalues of the eigenvectors. Roughly speaking, the eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data, and those are the ones we want to drop.\n",
    "The common approach is to rank the eigenvectors from highest to lowest corresponding eigenvalue and choose the top k eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eig_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3cdaf90aca2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make a list of (eigenvalue, eigenvector) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meig_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Sort the (eigenvalue, eigenvector) tuples from high to low\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meig_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eig_vals' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "\n",
    "print('Eigenvalues in decreasing order:\\n')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "If we take a look at the eigenvalues, we can already see that 2 eigenvalues are close to 0. The reason why these are close to 0 is not that they are not informative but it’s due to floating-point imprecision. In fact, these two last eigenvalues should be exactly zero: In LDA, the number of linear discriminants is at most c−1 where c is the number of class labels, since the in-between scatter matrix SB is the sum of c matrices with rank 1 or less. Note that in the rare case of perfect collinearity (all aligned sample points fall on a straight line), the covariance matrix would have rank one, which would result in only one eigenvector with a nonzero eigenvalue.\n",
    "\n",
    "Now, let’s express the “explained variance” as percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eig_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9f35e2e509a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variance explained:\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meigv_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eigenvalue {0:}: {1:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0meigv_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eig_vals' is not defined"
     ]
    }
   ],
   "source": [
    "print('Variance explained:\\n')\n",
    "eigv_sum = sum(eig_vals)\n",
    "for i,j in enumerate(eig_pairs):\n",
    "    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first eigenpair is by far the most informative one, and we won’t loose much information if we would form a 1D-feature spaced based on this eigenpair.\n",
    "\n",
    "### 4.2. Choosing k eigenvectors with the largest eigenvalues\n",
    "After sorting the eigenpairs by decreasing eigenvalues, it is now time to construct our k×d-dimensional eigenvector matrix WW (here 4×2: based on the 2 most informative eigenpairs) and thereby reducing the initial 4-dimensional feature space into a 2-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:\\n', W.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Transforming the samples onto the new subspace\n",
    "### In the last step, we use the 4×2-dimensional matrix WW that we just computed to transform our samples onto the new subspace via the equation\n",
    "\n",
    "YY=XX×WW.\n",
    "\n",
    "(where XX is a n×d-dimensional matrix representing the n samples, and YY are the transformed n×k-dimensional samples in the new subspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c68c8de21474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mX_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The matrix is not 150x2 dimensional.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X_lda = X.dot(W)\n",
    "assert X_lda.shape == (150,2), \"The matrix is not 150x2 dimensional.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8b30f926490e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mplot_step_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-8b30f926490e>\u001b[0m in \u001b[0;36mplot_step_lda\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         plt.scatter(x=X_lda[:,0].real[y == label],\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_lda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_lda' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_step_lda():\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X_lda[:,0].real[y == label],\n",
    "                y=X_lda[:,1].real[y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "\n",
    "plot_step_lda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot above represents our new feature subspace that we constructed via LDA. We can see that the first linear discriminant “LD1” separates the classes quite nicely. However, the second discriminant, “LD2”, does not add much valuable information, which we’ve already concluded when we looked at the ranked eigenvalues is step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison of PCA and LDA\n",
    "In order to compare the feature subspace that we obtained via the Linear Discriminant Analysis, we will use the PCA class from the scikit-learn machine-learning library. The documentation can be found here:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.\n",
    "\n",
    "For our convenience, we can directly specify to how many components we want to retain in our input dataset via the n_components parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-b24daba3d221>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-b24daba3d221>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    n_components : int, None or string\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_components : int, None or string\n",
    "\n",
    "Number of components to keep. if n_components is not set all components are kept:\n",
    "    n_components == min(n_samples, n_features)\n",
    "    if n_components == ‘mle’, Minka’s MLE is used to guess the dimension if 0 < n_components < 1,\n",
    "    select the number of components such that the amount of variance that needs to be explained\n",
    "    is greater than the percentage specified by n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we skip to the results of the respective linear transformations, let us quickly recapitulate the purposes of PCA and LDA: PCA finds the axes with maximum variance for the whole data set where LDA tries to find the axes for best class seperability. In practice, often a LDA is done followed by a PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=2)\n",
    "X_pca = sklearn_pca.fit_transform(X)\n",
    "\n",
    "def plot_pca():\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X_pca[:,0][y == label],\n",
    "                y=X_pca[:,1][y == label],\n",
    "                marker=marker,\n",
    "                color=color,\n",
    "                alpha=0.5,\n",
    "                label=label_dict[label]\n",
    "                )\n",
    "\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title('PCA: Iris projection onto the first 2 principal components')\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca()\n",
    "plot_step_lda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots above nicely confirm what we have discussed before: Where the PCA accounts for the most variance in the whole dataset, the LDA gives us the axes that account for the most variance between the individual classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA via scikit-learn\n",
    "Now, after we have seen how an Linear Discriminant Analysis works using a step-by-step approach, there is also a more convenient way to achive the same via the LDA class implemented in the scikit-learn machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# LDA\n",
    "sklearn_lda = LDA(n_components=2)\n",
    "X_lda_sklearn = sklearn_lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scikit_lda(X, title):\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    for label,marker,color in zip(\n",
    "        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "\n",
    "        plt.scatter(x=X[:,0][y == label],\n",
    "                    y=X[:,1][y == label] * -1, # flip the figure\n",
    "                    marker=marker,\n",
    "                    color=color,\n",
    "                    alpha=0.5,\n",
    "                    label=label_dict[label])\n",
    "\n",
    "    plt.xlabel('LD1')\n",
    "    plt.ylabel('LD2')\n",
    "\n",
    "    leg = plt.legend(loc='upper right', fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.5)\n",
    "    plt.title(title)\n",
    "\n",
    "    # hide axis ticks\n",
    "    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_step_lda()\n",
    "plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note About Standardization\n",
    "To follow up on a question that I received recently, I wanted to clarify that feature scaling such as [standardization] does not change the overall results of an LDA and thus may be optional. Yes, the scatter matrices will be different depending on whether the features were scaled or not. In addition, the eigenvectors will be different as well. However, the important part is that the eigenvalues will be exactly the same as well as the final projects – the only difference you’ll notice is the scaling of the component axes. This can be shown mathematically (I will insert the formulaes some time in future), and below is a practical, visual example for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "df[4] = df[4].map({'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we are going to standardize the columns in X. Standardization implies mean centering and scaling to unit variance:\n",
    "\n",
    "xstd=x−μxσX\n",
    "After standardization, the columns will have zero mean ( μxstd=0 ) and a standard deviation of 1 (σxstd=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = df.iloc[:, 4].values, df.iloc[:, 0:4].values\n",
    "X_cent = X - X.mean(axis=0)\n",
    "X_std = X_cent / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I simply copied the individual steps of an LDA, which we discussed previously, into Python functions for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def comp_mean_vectors(X, y):\n",
    "    class_labels = np.unique(y)\n",
    "    n_classes = class_labels.shape[0]\n",
    "    mean_vectors = []\n",
    "    for cl in class_labels:\n",
    "        mean_vectors.append(np.mean(X[y==cl], axis=0))\n",
    "    return mean_vectors\n",
    "\n",
    "def scatter_within(X, y):\n",
    "    class_labels = np.unique(y)\n",
    "    n_classes = class_labels.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    mean_vectors = comp_mean_vectors(X, y)\n",
    "    S_W = np.zeros((n_features, n_features))\n",
    "    for cl, mv in zip(class_labels, mean_vectors):\n",
    "        class_sc_mat = np.zeros((n_features, n_features))                 \n",
    "        for row in X[y == cl]:\n",
    "            row, mv = row.reshape(n_features, 1), mv.reshape(n_features, 1)\n",
    "            class_sc_mat += (row-mv).dot((row-mv).T)\n",
    "        S_W += class_sc_mat                           \n",
    "    return S_W\n",
    "\n",
    "def scatter_between(X, y):\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    n_features = X.shape[1]\n",
    "    mean_vectors = comp_mean_vectors(X, y)    \n",
    "    S_B = np.zeros((n_features, n_features))\n",
    "    for i, mean_vec in enumerate(mean_vectors):  \n",
    "        n = X[y==i+1,:].shape[0]\n",
    "        mean_vec = mean_vec.reshape(n_features, 1)\n",
    "        overall_mean = overall_mean.reshape(n_features, 1)\n",
    "        S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "    return S_B\n",
    "\n",
    "def get_components(eig_vals, eig_vecs, n_comp=2):\n",
    "    n_features = X.shape[1]\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "    eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "    W = np.hstack([eig_pairs[i][1].reshape(4, 1) for i in range(0, n_comp)])\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to print the eigenvalues, eigenvectors, transformation matrix of the un-scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W, S_B = scatter_within(X, y), scatter_between(X, y)\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "W = get_components(eig_vals, eig_vecs, n_comp=2)\n",
    "print('EigVals: %s\\n\\nEigVecs: %s' % (eig_vals, eig_vecs))\n",
    "print('\\nW: %s' % W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = X.dot(W)\n",
    "for label,marker,color in zip(\n",
    "        np.unique(y),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "    plt.scatter(X_lda[y==label, 0], X_lda[y==label, 1],\n",
    "                color=color, marker=marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are repeating this process for the standarized flower dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_W, S_B = scatter_within(X_std, y), scatter_between(X_std, y)\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "W_std = get_components(eig_vals, eig_vecs, n_comp=2)\n",
    "print('EigVals: %s\\n\\nEigVecs: %s' % (eig_vals, eig_vecs))\n",
    "print('\\nW: %s' % W_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std_lda = X_std.dot(W_std)\n",
    "X_std_lda[:, 1] = X_std_lda[:, 1]\n",
    "for label,marker,color in zip(\n",
    "        np.unique(y),('^', 's', 'o'),('blue', 'red', 'green')):\n",
    "    plt.scatter(X_std_lda[y==label, 0], X_std_lda[y==label, 1],\n",
    "                color=color, marker=marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, the eigenvalues are excactly the same whether we scaled our data or not (note that since W has a rank of 2, the two lowest eigenvalues in this 4-dimensional dataset should effectively be 0). Furthermore, we see that the projections look identical except for the different scaling of the component axes and that it is mirrored in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
